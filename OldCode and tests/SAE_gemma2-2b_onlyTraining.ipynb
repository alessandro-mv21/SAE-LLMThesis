{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install 'torch==2.5.1' einops datasets jaxtyping sae_lens transformer_lens openai tabulate \"nbformat>=4.2.0\" umap-learn hdbscan eindex-callum git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python git+https://github.com/callummcdougall/sae_vis.git@callum/v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni per vedere la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch as t\n",
    "from openai import OpenAIError\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def get_tensor_size(obj):\n",
    "    size = 0\n",
    "    if t.is_tensor(obj):\n",
    "        size += obj.element_size() * obj.nelement()\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_tensors_size(obj):\n",
    "    if isinstance(obj, t.nn.Module):\n",
    "        return sum(get_tensor_size(p) for p in obj.parameters())\n",
    "    if hasattr(obj, \"state_dict\"):\n",
    "        return sum(get_tensor_size(t) for t in obj.state_dict().values())\n",
    "    return get_tensor_size(obj)\n",
    "\n",
    "\n",
    "def get_device(obj):\n",
    "    if t.is_tensor(obj):\n",
    "        return str(obj.device)\n",
    "    if isinstance(obj, t.nn.Module):\n",
    "        try:\n",
    "            return str(next(iter(obj.parameters())).device)\n",
    "        except StopIteration:\n",
    "            return \"N/A\"\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "def print_memory_status():\n",
    "    t.cuda.synchronize()\n",
    "    allocated = t.cuda.memory_allocated(0)\n",
    "    total = t.cuda.get_device_properties(0).total_memory\n",
    "    free = total - allocated\n",
    "    print(f\"Allocated: {allocated / 1024**3:.2f} GB\")\n",
    "    print(f\"Total:  {total / 1024**3:.2f} GB\")\n",
    "    print(f\"Free:  {free / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "def profile_pytorch_memory(namespace: dict, n_top: int = 10, filter_device: str = None):\n",
    "    print_memory_status()\n",
    "\n",
    "    object_sizes = {}\n",
    "    for name, obj in namespace.items():\n",
    "        try:\n",
    "            obj_type = (\n",
    "                type(obj).__name__\n",
    "                if isinstance(obj, t.nn.Module)\n",
    "                else f\"Tensor {tuple(obj.shape)}\"\n",
    "                if t.is_tensor(obj)\n",
    "                else None\n",
    "            )\n",
    "            if obj_type is None:\n",
    "                continue\n",
    "            device = get_device(obj)\n",
    "            if filter_device and device != filter_device:\n",
    "                continue\n",
    "            size = get_tensors_size(obj)\n",
    "            object_sizes[name] = (obj_type, device, size / (1024**3))\n",
    "        except (OpenAIError, ReferenceError):\n",
    "            # OpenAIError: we can't inspect the type of certain objects without triggering API request\n",
    "            # ReferenceError: this object might have been garbage collected, so we don't care about it\n",
    "            continue\n",
    "\n",
    "    # Convert bytes to GB, sort by size & print\n",
    "    sorted_sizes = sorted(object_sizes.items(), key=lambda x: x[1][2], reverse=True)[:n_top]\n",
    "    table_data = [(name, obj_type, device, size) for name, (obj_type, device, size) in sorted_sizes]\n",
    "    print(\n",
    "        tabulate(\n",
    "            table_data, headers=[\"Name\", \"Object\", \"Device\", \"Size (GB)\"], floatfmt=\".2f\", tablefmt=\"simple_outline\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def find_cuda_tensors():\n",
    "    cuda_tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if t.is_tensor(obj) and obj.is_cuda:\n",
    "                cuda_tensors.append(obj)\n",
    "        except:\n",
    "            pass\n",
    "    return cuda_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altre utilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def make_square(num: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Trova i due fattori più vicini alla radice quadrata di un numero.\n",
    "\n",
    "    Args:\n",
    "        numero: Il numero intero di cui trovare i fattori.\n",
    "\n",
    "    Returns:\n",
    "        Una tupla contenente i due fattori più vicini alla radice quadrata.\n",
    "    \"\"\"\n",
    "\n",
    "    if num < 1:\n",
    "        raise ValueError(\"Il numero deve essere maggiore o uguale a 1.\")\n",
    "\n",
    "    radice_quadrata = math.sqrt(num)\n",
    "    fattore_inferiore = math.floor(radice_quadrata)\n",
    "    fattore_superiore = math.ceil(radice_quadrata)\n",
    "\n",
    "    while num % fattore_inferiore != 0:\n",
    "        fattore_inferiore -= 1\n",
    "\n",
    "    while num % fattore_superiore != 0:\n",
    "        fattore_superiore += 1\n",
    "\n",
    "    return fattore_inferiore, fattore_superiore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import torch as t\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Total:  6.00 GB\n",
      "Free:  6.00 GB\n",
      "┌────────┬──────────┬──────────┬─────────────┐\n",
      "│ Name   │ Object   │ Device   │ Size (GB)   │\n",
      "├────────┼──────────┼──────────┼─────────────┤\n",
      "└────────┴──────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Profile memory usage, and delete gemma models if we've loaded them in\n",
    "namespace = globals().copy() | locals()\n",
    "profile_pytorch_memory(namespace=namespace, filter_device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training del SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per i parametri riporto i config dei SAE allenati da google per gemma-scope-2b. \n",
    "Dall'HF google/gemma-scope-2b-pt-res:\\\n",
    "What Is gemma-scope-2b-pt-res?\n",
    "* gemma-scope-: Gemma Scope is a comprehensive, open suite of sparse autoencoders for Gemma 2 9B and 2B. Sparse Autoencoders are a \"microscope\" of sorts that can help us break down a model’s internal activations into the underlying concepts, just as biologists use microscopes to study the individual cells of plants and animals.\n",
    "* 2b-pt-: These SAEs were trained on Gemma v2 2B base model.\n",
    "* res: These SAEs were trained on the model's residual stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendiamo un SAE di gemmascope. Per esempio quello che viene usato su neuronpedia per vederne i config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"gemma-scope-2b-pt-res\"\n",
    "sae_id = \"layer_20/width_16k/average_l0_71\"\n",
    "sae_gemmascope_cfg = SAE.from_pretrained(release, sae_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.20.hook_resid_post',\n",
       " 'hook_layer': 20,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': 'cpu',\n",
       " 'neuronpedia_id': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gemmascope_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.111111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gemmascope_cfg['d_sae']/sae_gemmascope_cfg['d_in']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traineremo un SAE sul livello 20. Dal paper https://arxiv.org/pdf/2408.05147 troviamo i token usati, ovvero 4B per una dimensione del SAE di circa 16k\n",
    "La batch size usata è 4096 e la learning rate 7e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametri importanti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training tokens: 1,228,800,000\n",
      "Total training steps: 300,000\n",
      "Learning rate: 7e-05\n",
      "Logging in to WANDB\n",
      "tokens per step: 4194304\n"
     ]
    }
   ],
   "source": [
    "# parametri del training\n",
    "total_training_steps = 300_000  # Calculated from training_tokens / batch_size\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "print(f\"Total training tokens: {total_training_tokens:,}\")\n",
    "print(f\"Total training steps: {total_training_steps:,}\")\n",
    "learning_rate = 7e-5\n",
    "print(F\"Learning rate: {learning_rate}\")\n",
    "\n",
    "lr_warm_up_steps = l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "\n",
    "# wandb\n",
    "\n",
    "log_to_wandb = True\n",
    "print(\"Logging in to WANDB\")\n",
    "\n",
    "# parametri del modello\n",
    "model_name = 'gemma-2-2b'\n",
    "hook_name = 'blocks.20.hook_resid_post'\n",
    "hook_layer = 20\n",
    "d_sae = 16384\n",
    "d_in = 2304\n",
    "\n",
    "l1_coefficient = 2\n",
    "\n",
    "context_size = 1024          # il pretrained ha 128\n",
    "\n",
    "print(f'tokens per step: {batch_size*context_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    #\n",
    "    # Data generation\n",
    "    model_name=model_name,\n",
    "    hook_name=hook_name,\n",
    "    hook_layer=hook_layer,\n",
    "    d_in=2304,\n",
    "    dataset_path=\"chanind/openwebtext-gemma\",\n",
    "    is_dataset_tokenized=True,\n",
    "    # dataset_path=\"HuggingFaceFW/fineweb\",\n",
    "    # is_dataset_tokenized=False,\n",
    "    prepend_bos=True,\n",
    "    streaming=True,\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=context_size,\n",
    "    #\n",
    "    # SAE architecture\n",
    "    architecture=\"jumprelu\",\n",
    "    d_sae=d_sae,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    apply_b_dec_to_input=True,\n",
    "    #\n",
    "    # Activations store\n",
    "    n_batches_in_buffer=32,\n",
    "    training_tokens=total_training_tokens,\n",
    "    store_batch_size_prompts=16,\n",
    "    #\n",
    "    # Training hyperparameters (standard)\n",
    "    lr=learning_rate,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    lr_warm_up_steps=lr_warm_up_steps,\n",
    "    lr_decay_steps=lr_decay_steps,\n",
    "    #\n",
    "    # Training hyperparameters (SAE-specific)\n",
    "    l1_coefficient=l1_coefficient,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=5000,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-6,\n",
    "    #\n",
    "    # Logging / evals\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"gemma2-2bL20-16k\",\n",
    "    wandb_log_frequency=50,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    #\n",
    "    # Misc.\n",
    "    device=str(device),\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(True)\n",
    "runner = SAETrainingRunner(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"Ale21m9/GEMMA2-2b-16k\"\n",
    "sae_id = cfg.hook_name\n",
    "\n",
    "upload_saes_to_huggingface({sae_id: sae}, hf_repo_id=hf_repo_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
